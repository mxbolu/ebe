#!/usr/bin/env tsx
/**
 * Optimized Bulk Import Script for Books
 *
 * Key optimizations:
 * - Batch inserts using createMany
 * - In-memory duplicate tracking
 * - Parallel subject processing
 * - Reduced database queries
 *
 * Usage:
 *   npm run import:books:fast -- --source=openlibrary --limit=1000000
 */

import 'dotenv/config'
import { PrismaClient } from '@prisma/client'
import { PrismaPg } from '@prisma/adapter-pg'
import { Pool } from 'pg'
import { openLibraryService } from '../lib/services/openLibrary'
import { googleBooksService, GoogleBooksService } from '../lib/services/googleBooks'

// Create Prisma Client with adapter for Prisma Postgres
const connectionString = process.env.DATABASE_URL || ''
let pgConnectionString = connectionString

if (connectionString.startsWith('prisma+postgres://')) {
  const url = new URL(connectionString)
  const apiKey = url.searchParams.get('api_key')
  if (apiKey) {
    const decoded = JSON.parse(Buffer.from(apiKey, 'base64').toString())
    pgConnectionString = decoded.databaseUrl
  }
}

const pool = new Pool({ connectionString: pgConnectionString })
const adapter = new PrismaPg(pool)
const prisma = new PrismaClient({ adapter })

interface ImportOptions {
  source: 'openlibrary' | 'googlebooks'
  limit?: number
  batchSize?: number
  parallelSubjects?: number
}

class OptimizedBulkImporter {
  private options: ImportOptions
  private jobId?: string
  private processedCount = 0
  private successCount = 0
  private errorCount = 0
  private duplicateCount = 0

  // In-memory cache for duplicate detection
  private seenISBNs = new Set<string>()
  private seenOpenLibraryIds = new Set<string>()
  private seenGoogleBooksIds = new Set<string>()

  // Buffer for batch inserts
  private insertBuffer: any[] = []
  private readonly INSERT_BATCH_SIZE = 500

  constructor(options: ImportOptions) {
    this.options = {
      batchSize: 100,
      parallelSubjects: 3,
      ...options,
    }
  }

  async start() {
    console.log('ðŸš€ Starting optimized bulk book import...')
    console.log(`Source: ${this.options.source}`)
    console.log(`Limit: ${this.options.limit || 'unlimited'}`)
    console.log(`Batch size: ${this.options.batchSize}`)
    console.log(`Parallel subjects: ${this.options.parallelSubjects}\n`)

    await this.createJob()
    await this.loadExistingIdentifiers()

    if (this.options.source === 'openlibrary') {
      await this.importFromOpenLibrary()
    } else {
      await this.importFromGoogleBooks()
    }

    // Flush remaining buffer
    await this.flushBuffer()

    await this.completeJob()
    await prisma.$disconnect()
  }

  private async createJob() {
    const job = await prisma.importJob.create({
      data: {
        source: this.options.source,
        status: 'running',
        totalRecords: this.options.limit || 0,
        startedAt: new Date(),
      },
    })

    this.jobId = job.id
    console.log(`ðŸ“ Created import job: ${this.jobId}\n`)
  }

  private async loadExistingIdentifiers() {
    console.log('ðŸ“‹ Loading existing book identifiers into memory...')

    const batchSize = 10000
    let offset = 0
    let hasMore = true

    while (hasMore) {
      const books = await prisma.book.findMany({
        select: {
          isbn: true,
          openLibraryId: true,
          googleBooksId: true,
        },
        take: batchSize,
        skip: offset,
      })

      if (books.length === 0) {
        hasMore = false
        break
      }

      books.forEach((book) => {
        if (book.isbn) this.seenISBNs.add(book.isbn)
        if (book.openLibraryId) this.seenOpenLibraryIds.add(book.openLibraryId)
        if (book.googleBooksId) this.seenGoogleBooksIds.add(book.googleBooksId)
      })

      offset += batchSize
    }

    console.log(`âœ… Loaded ${this.seenISBNs.size} ISBNs, ${this.seenOpenLibraryIds.size} Open Library IDs, ${this.seenGoogleBooksIds.size} Google Books IDs\n`)
  }

  private async importFromOpenLibrary() {
    console.log('ðŸ“š Fetching books from Open Library...\n')

    const subjects = require('../lib/services/openLibrary').OpenLibraryService.POPULAR_SUBJECTS
    const parallelSubjects = this.options.parallelSubjects!

    // Process subjects in parallel chunks
    for (let i = 0; i < subjects.length; i += parallelSubjects) {
      const subjectBatch = subjects.slice(i, i + parallelSubjects)

      await Promise.all(
        subjectBatch.map((subject: string) => this.processSubject(subject))
      )

      if (this.options.limit && this.processedCount >= this.options.limit) {
        console.log(`\nðŸŽ¯ Reached limit of ${this.options.limit} books`)
        break
      }
    }
  }

  private async processSubject(subject: string) {
    console.log(`ðŸ“– Processing subject: ${subject}`)

    let offset = 0
    let hasMore = true

    while (hasMore && (!this.options.limit || this.processedCount < this.options.limit)) {
      try {
        const response = await require('axios').default.get('https://openlibrary.org/search.json', {
          params: {
            subject,
            limit: this.options.batchSize,
            offset,
            fields: 'key,title,author_name,first_publish_year,isbn,publisher,subject,cover_i,number_of_pages_median',
          },
          timeout: 30000,
          headers: {
            'User-Agent': 'ebe-reading-journal/1.0',
          },
        })

        const docs = response.data.docs || []
        if (docs.length === 0) {
          hasMore = false
          break
        }

        // Process books
        for (const doc of docs) {
          this.processedCount++

          const book = {
            openLibraryId: doc.key,
            title: doc.title || 'Unknown Title',
            authors: doc.author_name || ['Unknown Author'],
            isbn: doc.isbn?.[0],
            publishedYear: doc.first_publish_year,
            genres: (doc.subject || []).slice(0, 5),
            pageCount: doc.number_of_pages_median,
            publisher: doc.publisher?.[0],
            coverUrl: doc.cover_i ? `https://covers.openlibrary.org/b/id/${doc.cover_i}-M.jpg` : undefined,
          }

          if (this.isDuplicate(book)) {
            this.duplicateCount++
            continue
          }

          this.addToBuffer(book)
        }

        offset += docs.length

        if (docs.length < this.options.batchSize!) {
          hasMore = false
        }

        // Rate limiting
        await new Promise((resolve) => setTimeout(resolve, 1000))
      } catch (error: any) {
        console.error(`Error fetching subject "${subject}":`, error.message)
        hasMore = false
      }
    }
  }

  private async importFromGoogleBooks() {
    console.log('ðŸ“š Fetching books from Google Books...\n')

    const subjects = GoogleBooksService.POPULAR_SUBJECTS

    for await (const batch of googleBooksService.fetchBulkBySubject(
      subjects[0],
      this.options.limit || 100000
    )) {
      if (this.options.limit && this.processedCount >= this.options.limit) {
        break
      }

      for (const book of batch) {
        this.processedCount++

        if (this.isDuplicate(book)) {
          this.duplicateCount++
          continue
        }

        this.addToBuffer(book)
      }
    }
  }

  private isDuplicate(book: any): boolean {
    // Check in-memory cache
    if (book.isbn && this.seenISBNs.has(book.isbn)) return true
    if (book.openLibraryId && this.seenOpenLibraryIds.has(book.openLibraryId)) return true
    if (book.googleBooksId && this.seenGoogleBooksIds.has(book.googleBooksId)) return true

    // Add to cache
    if (book.isbn) this.seenISBNs.add(book.isbn)
    if (book.openLibraryId) this.seenOpenLibraryIds.add(book.openLibraryId)
    if (book.googleBooksId) this.seenGoogleBooksIds.add(book.googleBooksId)

    return false
  }

  private addToBuffer(book: any) {
    this.insertBuffer.push({
      title: book.title,
      authors: book.authors || [],
      isbn: book.isbn || null,
      coverImageUrl: book.coverUrl || null,
      description: book.description || null,
      publishedYear: book.publishedYear || null,
      genres: book.genres || [],
      pageCount: book.pageCount || null,
      publisher: book.publisher || null,
      language: book.language || 'en',
      source: 'API_IMPORT',
      openLibraryId: book.openLibraryId || null,
      googleBooksId: book.googleBooksId || null,
      lastSyncedAt: new Date(),
    })

    // Flush buffer when it reaches the batch size
    if (this.insertBuffer.length >= this.INSERT_BATCH_SIZE) {
      this.flushBuffer()
    }
  }

  private async flushBuffer() {
    if (this.insertBuffer.length === 0) return

    const buffer = [...this.insertBuffer]
    this.insertBuffer = []

    // Insert books individually (createMany not supported with adapter)
    let succeeded = 0
    for (const book of buffer) {
      try {
        await prisma.book.create({ data: book })
        succeeded++
      } catch (err) {
        // Likely a duplicate, silently skip
        this.duplicateCount++
      }
    }

    this.successCount += succeeded
    await this.updateProgress()
    this.printProgress()
  }

  private async updateProgress() {
    if (!this.jobId) return

    await prisma.importJob.update({
      where: { id: this.jobId },
      data: {
        processedRecords: this.processedCount,
        successCount: this.successCount,
        errorCount: this.errorCount,
      },
    })
  }

  private printProgress() {
    const successRate = this.processedCount > 0
      ? ((this.successCount / this.processedCount) * 100).toFixed(2)
      : '0.00'

    process.stdout.write(
      `\rðŸ“Š Processed: ${this.processedCount} | Success: ${this.successCount} | Duplicates: ${this.duplicateCount} | Errors: ${this.errorCount} | ${successRate}% success`
    )
  }

  private async completeJob() {
    if (!this.jobId) return

    await prisma.importJob.update({
      where: { id: this.jobId },
      data: {
        status: 'completed',
        completedAt: new Date(),
        processedRecords: this.processedCount,
        successCount: this.successCount,
        errorCount: this.errorCount,
      },
    })

    console.log('\n\nâœ… Import completed successfully!')
    console.log(`ðŸ“Š Total processed: ${this.processedCount}`)
    console.log(`âœ… Successful imports: ${this.successCount}`)
    console.log(`ðŸ”„ Duplicates skipped: ${this.duplicateCount}`)
    console.log(`âŒ Errors: ${this.errorCount}`)
  }
}

// Parse command line arguments
const args = process.argv.slice(2)
const options: ImportOptions = {
  source: 'openlibrary',
}

args.forEach((arg) => {
  const [key, value] = arg.split('=')
  if (key === '--source') options.source = value as 'openlibrary' | 'googlebooks'
  if (key === '--limit') options.limit = parseInt(value)
  if (key === '--batch-size') options.batchSize = parseInt(value)
  if (key === '--parallel-subjects') options.parallelSubjects = parseInt(value)
})

const importer = new OptimizedBulkImporter(options)
importer.start().catch((error) => {
  console.error('Import failed:', error)
  process.exit(1)
})
